# -*- coding: utf-8 -*-
"""Testing_inference_speed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-YzHtEJDB60RrVgIOxc3ItQX1wv_izA
"""

test_prompts = [
    # 1. Simple Case: Two clear tasks, two owners.
    """#Person1#: Morning team. For the Apollo launch, I need two things. Jen, can you please finalize the press release by EOD?
#Person2#: On it.
#Person1#: Thanks. And Mike, can you get the server resource report to me by 3 PM today?
#Person3#: Will do.""",

    # 2. Negative Case: A conversation with no action items.
    """#Person1#: Hey, did you see the game last night?
#Person2#: Yeah, incredible comeback in the final quarter!
#Person1#: I thought so too. Anyway, just wanted to check in.
#Person2#: Sounds good, catch you later.""",

    # 3. Implicit Task: The task is implied, not directly stated.
    """#Person1#: The project's documentation is getting really out of date. It's becoming hard for new hires to follow.
#Person2#: I agree. I have some bandwidth this week, I can take a pass at updating the core modules.
#Person1#: That would be a huge help, thanks Sarah.""",

    # 4. Single Person, Multiple Tasks: One person is assigned several duties.
    """#Person1#: David, I need a hand with the new client onboarding.
#Person2#: Sure, what do you need?
#Person1#: Can you please schedule the kickoff meeting, prepare the initial slide deck, and also send them the security questionnaire? The deck should be done by Wednesday.
#Person2#: I can get all that done.""",

    # 5. Vague Deadline & Chit-Chat: Contains conversational noise and a non-specific deadline.
    """#Person1#: Hey Lisa, how was your vacation?
#Person2#: It was amazing, thanks! Back to reality now. What's on fire?
#Person1#: Nothing major, but can you look into the bug reports from the last sprint? Just need it done sometime this week.
#Person2#: No problem, I'll add it to my list.""",

    # 6. Updating an Existing Task: The deadline is changed mid-conversation.
    """#Person1#: Mark, did you get the analytics report done?
#Person2#: Almost, I'll have it to you by the end of the day.
#Person1#: Actually, can you prioritize that? I need it before the 1 PM sync with the execs.
#Person2#: Okay, shifting gears. I'll get it to you by 12:30 PM.""",

    # 7. Group Assignment: The task is assigned to a team, not an individual.
    """#Manager#: Alright team, great work this quarter. Looking ahead, the entire marketing team needs to submit their Q4 budget proposals by next Friday.
#Team-Member#: Got it, we'll coordinate and submit one unified proposal.""",

    # 8. Conditional Task: An action that depends on another event.
    """#Person1#: I've sent the design mockups to the client for feedback.
#Person2#: Great. If they approve them, I'll start building out the front-end components.
#Person1#: Perfect. Hopefully we hear back by tomorrow.""",

    # 9. Simple Rejection: An assigned task is refused or postponed.
    """#Leader#: Can someone take on the task of refactoring the old payment module?
#Dev1#: I'm swamped with the checkout bug.
#Dev2#: My plate is full too for the next two weeks. We might need to table this.
#Leader#: Understood.""",

    # 10. Complex/Long Conversation: A multi-turn conversation where only one real action item emerges.
    """#ProjectManager#: Let's discuss the release blocker. The memory leak is still happening.
#Engineer1#: I've been investigating. I think it's in the caching service, but it's hard to replicate.
#ProjectManager#: How long do you think it'll take to find the root cause?
#Engineer2#: I can help. I've seen this before. Let me spend the rest of the day on it, I'm confident I can isolate it.
#ProjectManager#: That would be fantastic, Ben. Please keep us updated.
#Engineer1#: Thanks Ben, that's a huge help."""
]

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# Ensure your model and adapter paths are correct
adapter_path = "/content/phi-4-4B-action-extractor-adapter" # UPDATE THIS PATH

# --- Load Model and Tokenizer (same as before) ---
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16)
base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval() # Set the model to evaluation mode

# --- Benchmarking Loop ---
inference_times = []

# Perform a single warm-up run (important for accurate timing)
print("Performing warm-up run...")
warmup_prompt = "Hello, this is a warm-up."
_ = model.generate(**tokenizer(warmup_prompt, return_tensors="pt").to("cuda"), max_new_tokens=5)
print("Warm-up complete. Starting benchmark.\n" + "="*50)

for i, conversation in enumerate(test_prompts):
    print(f"\n--- Test Case {i+1} ---")

    # Format the prompt
    prompt = f"""### Instruction:
Extract the action items from the following conversation and format them as a JSON object.

### Conversation:
{conversation}

### Response:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Time the inference
    start_time = time.time()
    outputs = model.generate(**inputs, max_new_tokens=250, temperature=0.1, pad_token_id=tokenizer.eos_token_id)
    end_time = time.time()

    # Store the duration
    duration = end_time - start_time
    inference_times.append(duration)

    # Decode and print the output
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    try:
        response_json = response.split("### Response:")[1].strip()
        print(response_json)
    except IndexError:
        print("Model did not generate a response in the expected format.")
        print(f"Full output: {response}")

    print(f"Time taken: {duration:.4f} seconds")

# --- Calculate and Print Average Inference Time ---
average_time = sum(inference_times) / len(inference_times)
print("\n" + "="*50)
print(f"Benchmark Complete.")
print(f"Average inference time over {len(test_prompts)} runs: {average_time:.4f} seconds")
print("="*50)