# -*- coding: utf-8 -*-
"""Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-YzHtEJDB60RrVgIOxc3ItQX1wv_izA
"""

import time

# Load the fine-tuned adapter
peft_model = PeftModel.from_pretrained(model, "/content/phi-4-4B-action-extractor-adapter")

# The conversation we want to extract from
conversation_to_test = """#Person1#: Hey team, let's sync on the Q3 report.
#Person2#: I can take the lead on gathering the sales data. I'll have it done by Friday.
#Person1#: Great, thanks Alex. Maria, can you handle the marketing metrics?
#Person3#: Sure thing. I'll get those to you by end of day Thursday.
"""

# Format the prompt EXACTLY as we did in the dataset
inference_prompt = f"""### Instruction:
Extract the action items from the following conversation and format them as a JSON object.

### Conversation:
{conversation_to_test}

### Response:
"""

# Tokenize and generate
inputs = tokenizer(inference_prompt, return_tensors="pt").to("cuda")
outputs = peft_model.generate(**inputs, max_new_tokens=250)

# Decode and print
print(tokenizer.decode(outputs[0], skip_special_tokens=True))