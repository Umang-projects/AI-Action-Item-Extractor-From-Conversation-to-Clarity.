# -*- coding: utf-8 -*-
"""Custom_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-YzHtEJDB60RrVgIOxc3ItQX1wv_izA
"""

from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling

class MyCustomDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # Format the prompt
        input_text = item['prompt']
        output_text = item['completion']
        instruction_part = f"""### Instruction:
Extract the action items from the following conversation and format them as a JSON object.

### Conversation:
{input_text}

### Response:
"""
        full_text = instruction_part + output_text + self.tokenizer.eos_token

        # Tokenize the full text with padding and truncation
        tokenized_full = self.tokenizer(
            full_text,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
        )

        # Tokenize the instruction part separately to find its length
        tokenized_instruction = self.tokenizer(
            instruction_part,
            max_length=self.max_length,
            truncation=True,
        )

        # Create labels by copying input_ids
        labels = list(tokenized_full['input_ids'])

        # Calculate the length of the instruction part
        instruction_len = len(tokenized_instruction['input_ids'])

        # Mask the instruction tokens in the labels with -100
        if instruction_len < len(labels):
            labels[:instruction_len] = [-100] * instruction_len
        else:
            labels = [-100] * len(labels)

        # Add labels to the dictionary
        tokenized_full["labels"] = labels
        return tokenized_full

# Instantiate the dataset
train_dataset = MyCustomDataset(dataset['train'], tokenizer)